# -*- coding: utf-8 -*-
"""lg_aimers

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q4Vl6-CxVrpKK6jPx96nlkdbySlXpXxo
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# GPU ì„¤ì •
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
if tf.config.experimental.list_physical_devices('GPU'):
    tf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0], True)

data_path = "/content/drive/MyDrive/lg_data/train.csv"
test_path = "/content/drive/MyDrive/lg_data/test.csv"

df = pd.read_csv(data_path, encoding="utf-8-sig")
test_df = pd.read_csv(test_path, encoding="utf-8-sig")

df.columns = df.columns.str.strip()
test_df.columns = test_df.columns.str.strip()

# ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
drop_columns = ["ID", "ì‹œìˆ  ì‹œê¸° ì½”ë“œ"]
df = df.drop(columns=drop_columns)
test_df = test_df.drop(columns=drop_columns)

print(df.columns.tolist())  # ì»¬ëŸ¼ëª…ì„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì¶œë ¥

from sklearn.preprocessing import LabelEncoder

# ë²”ì£¼í˜• ë°ì´í„° ì»¬ëŸ¼ ëª©ë¡ í™•ì¸
categorical_columns = df.select_dtypes(include=['object']).columns.tolist()

# LabelEncoder ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
label_encoders = {}

for col in categorical_columns:
    le = LabelEncoder()

    # í›ˆë ¨ ë°ì´í„°ë¡œ Label Encoding í•™ìŠµ
    df[col] = le.fit_transform(df[col].astype(str))
    label_encoders[col] = le

    # ğŸ›  **í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜ ì‹œ ì˜ˆì™¸ ì²˜ë¦¬**
    test_df[col] = test_df[col].astype(str).apply(lambda x: x if x in le.classes_ else "Unknown")

    # ìƒˆë¡œìš´ ê°’ì´ ìƒê¸°ë©´ ê¸°ì¡´ classesì— ì¶”ê°€ í›„ ë³€í™˜
    le.classes_ = np.append(le.classes_, "Unknown")
    test_df[col] = le.transform(test_df[col])

# "ì„ì‹  ì„±ê³µ ì—¬ë¶€" ì»¬ëŸ¼ì´ í›ˆë ¨ ë°ì´í„°(df)ì—ëŠ” ìˆì§€ë§Œ, í…ŒìŠ¤íŠ¸ ë°ì´í„°(test_df)ì—ëŠ” ì—†ìŒ
# âœ… X_train, y_train ë¶„ë¦¬
X_train = df.drop(columns=["ì„ì‹  ì„±ê³µ ì—¬ë¶€"])  # ì…ë ¥ ë°ì´í„°
y_train = df["ì„ì‹  ì„±ê³µ ì—¬ë¶€"]  # íƒ€ê²Ÿ ë³€ìˆ˜

# âœ… X_testëŠ” test_dfì—ì„œ ë™ì¼í•œ ì…ë ¥ ë°ì´í„°ë§Œ ì‚¬ìš©
X_test = test_df  # test_dfì—ëŠ” "ì„ì‹  ì„±ê³µ ì—¬ë¶€" ì»¬ëŸ¼ì´ ì—†ìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©

print("âœ… X_train ì»¬ëŸ¼ ëª©ë¡:", X_train.columns.tolist())
print("âœ… X_test ì»¬ëŸ¼ ëª©ë¡:", X_test.columns.tolist())  # ë¹„êµ í™•ì¸

from sklearn.preprocessing import LabelEncoder

# ë²”ì£¼í˜• ì»¬ëŸ¼ ì°¾ê¸°
categorical_columns = X_train.select_dtypes(include=['object']).columns.tolist()

# ê° ì»¬ëŸ¼ì— ëŒ€í•´ Label Encoding ìˆ˜í–‰
label_encoders = {}

for col in categorical_columns:
    le = LabelEncoder()

    # í›ˆë ¨ ë°ì´í„°ì— Label Encoding ì ìš©
    X_train[col] = le.fit_transform(X_train[col].astype(str))

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ë„ ë™ì¼í•œ Label Encoding ì ìš© (ì˜ˆì™¸ì²˜ë¦¬ í¬í•¨)
    X_test[col] = X_test[col].astype(str).apply(lambda x: x if x in le.classes_ else "Unknown")
    le.classes_ = np.append(le.classes_, "Unknown")  # ìƒˆë¡œìš´ ê°’ ì²˜ë¦¬
    X_test[col] = le.transform(X_test[col])

    # Label Encoder ì €ì¥
    label_encoders[col] = le

print("âœ… ë²”ì£¼í˜• ë°ì´í„° Label Encoding ì™„ë£Œ!")

from sklearn.preprocessing import StandardScaler

# ìˆ«ìí˜• ì»¬ëŸ¼ ì„ íƒ
numeric_columns = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()

# StandardScaler ì ìš©
scaler = StandardScaler()
X_train[numeric_columns] = scaler.fit_transform(X_train[numeric_columns])
X_test[numeric_columns] = scaler.transform(X_test[numeric_columns])  # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ ë™ì¼ ë³€í™˜

print("âœ… ë°ì´í„° ì •ê·œí™” ì™„ë£Œ!")

from sklearn.model_selection import train_test_split

# í›ˆë ¨ ë°ì´í„° (80%) / ê²€ì¦ ë°ì´í„° (20%) ë¶„í• 
X_train_final, X_val, y_train_final, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)

print("âœ… í›ˆë ¨ ë°ì´í„° í¬ê¸°:", X_train_final.shape)
print("âœ… ê²€ì¦ ë°ì´í„° í¬ê¸°:", X_val.shape)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# ëª¨ë¸ ì •ì˜
model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train_final.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1, activation='sigmoid')  # ì´ì§„ ë¶„ë¥˜ì´ë¯€ë¡œ sigmoid ì‚¬ìš©
])

# ëª¨ë¸ ì»´íŒŒì¼
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# EarlyStopping ì„¤ì • (ê³¼ì í•© ë°©ì§€)
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# ëª¨ë¸ í•™ìŠµ
model.fit(X_train_final, y_train_final,
          validation_data=(X_val, y_val),
          epochs=50, batch_size=32,
          callbacks=[early_stopping], verbose=1)

print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

# ê²€ì¦ ë°ì´í„° í‰ê°€
val_loss, val_acc = model.evaluate(X_val, y_val)
print(f"ê²€ì¦ ë°ì´í„° ì†ì‹¤ê°’: {val_loss:.4f}")
print(f"ê²€ì¦ ë°ì´í„° ì •í™•ë„(Accuracy): {val_acc:.4f}")

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ê°’ ê³„ì‚°
test_predictions = model.predict(X_test)
test_predictions = (test_predictions > 0.5).astype(int)

# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ê°’ ë¶„í¬ í™•ì¸
print("âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ê°’ ë¶„í¬:")
print(pd.Series(test_predictions.flatten()).value_counts())

# í›ˆë ¨ ë°ì´í„°ì—ì„œ ì˜ˆì¸¡ê°’ ìƒì„±
y_train_pred = model.predict(X_train)
y_train_pred = (y_train_pred > 0.5).astype(int)  # í™•ë¥ ì„ 0 ë˜ëŠ” 1ë¡œ ë³€í™˜

# í›ˆë ¨ ë°ì´í„°ì—ì„œ ì„±ê³µ(1)ì˜ ë¹„ìœ¨
train_success_ratio = y_train_pred.mean()

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ì„±ê³µ(1)ì˜ ë¹„ìœ¨
test_success_ratio = test_predictions.mean()

print(f"âœ… í›ˆë ¨ ë°ì´í„°ì—ì„œ ì„±ê³µ ë¹„ìœ¨: {train_success_ratio:.4f}")
print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ì„±ê³µ ë¹„ìœ¨: {test_success_ratio:.4f}")
print(f"âœ… ë‘ ë°ì´í„°ì…‹ì˜ ë¹„ìœ¨ ì°¨ì´: {abs(train_success_ratio - test_success_ratio):.4f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
plt.hist(y_train_pred, bins=2, alpha=0.5, label="í›ˆë ¨ ë°ì´í„° ì˜ˆì¸¡ê°’")
plt.hist(test_predictions, bins=2, alpha=0.5, label="í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ê°’")
plt.xticks([0, 1], ["ì„ì‹  ì‹¤íŒ¨(0)", "ì„ì‹  ì„±ê³µ(1)"])
plt.xlabel("ì˜ˆì¸¡ ê²°ê³¼")
plt.ylabel("ìƒ˜í”Œ ê°œìˆ˜")
plt.title("í›ˆë ¨ ë°ì´í„° vs í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ê°’ ë¹„êµ")
plt.legend()
plt.show()

